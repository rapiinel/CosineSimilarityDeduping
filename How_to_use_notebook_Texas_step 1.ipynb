{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import awesome_cossim_topn \n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from deduping_module import deduping_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = r'C:\\Users\\USER\\Documents\\LM_project\\SAL-230_Revisit validity'\n",
    "gt = pd.read_csv(link + '\\\\raw\\sf_export.csv', encoding='latin-1')\n",
    "nm = pd.read_csv(link + '\\\\Texas\\\\texas-00001 - step 1.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the module\n",
    "deduping = deduping_class(gt, 'account') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "1. Make sure that the ground truth columns are similar with the dataframe to be matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching the ground truth columns with the to match dataframe\n",
    "nm.rename(columns={'firstName':'First Name', 'lastName':'Last Name', 'email':'Email', 'phone':'Phone', 'zip':'Zip Code 1', 'First and Last':'Account Name', 'street':'Street Address 1', 'state':'State 1'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm.fillna('', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm[['Account Name', 'Street Address 1', 'State 1', 'Phone']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Now that the columns are matched, we can use the key_selector function. \\\n",
    "This takes 2 inputs, *args and \"data=dataframe to be matched\"\\\n",
    "This function will return the to be matched dataframe with primary_key value while also adding it in the ground truth but only in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.key_selector('Account Name', 'Street Address 1', 'State 1', 'Phone', data= nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduping.nm['State 1'] = deduping.nm.apply(lambda x: deduping.state_abbrev(x['State 1']), axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Step\n",
    "We can set an optional paramater called ngrams, this means the number of combination the txt will be divided. \\\n",
    "if this is not set, it will automatically equal to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.set_ngrams(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "This function will convert the ground truth and dataframe to be matched in to tfidf sparse matrix \\\n",
    "the input to this function should be the dataframe with the primarykey \\\n",
    "the results will now be included in the class module\n",
    "\n",
    "self.nm_tfidf\\\n",
    "self.gt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.nm_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.gt_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "This function will get compare the similarities in the 2 dataframe and save the output in self.match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.get_match(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.non_matched_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.matched_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduping.nm[~deduping.nm.index.isin(deduping.matched['index'].tolist())].to_csv('non_match_step 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduping.nm.merge(deduping.matched[['index','Ground Truth ID','similarity']].set_index('index'), left_index= True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "1. If deduping at account object, always remember to remove the duplicates in the Salesforce Account Id\n",
    "1. always make sure that the index of the inputs are in numerical order or this will cause errors in getting the matches\n",
    "1. if a selected key value is missing, it has a significant impact on the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(data=deduping.matched, x='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=deduping.matched, x='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.matched.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduping.matched.describe().loc['mean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chebyshev's Theorem limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower Limit\n",
    "lower_limit = deduping.matched.describe().loc['mean'][0] - (2 * deduping.matched.describe().loc['std'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upper_limit = deduping.matched.describe().loc['mean'][0] + (2 * deduping.matched.describe().loc['std'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deduping_performance:\n",
    "    def __init__(self, nm, deduping_matched):\n",
    "        self.df = nm.merge(deduping_matched[['index','Ground Truth ID','similarity']].set_index('index'), left_index= True, right_index=True)\n",
    "        self.df.sort_values(by='similarity', ascending= False, inplace= True)\n",
    "        self.df['similarity'] = self.df['similarity'].round(decimals= 3)\n",
    "        self.df.drop_duplicates(subset=['similarity'], inplace =True)\n",
    "        self.df.reset_index(drop=True, inplace= True)\n",
    "\n",
    "        if len(self.df) < 25:\n",
    "            raise ValueError('dataframe is less than threshold rows (25)')\n",
    "\n",
    "        self.confirmation_list = [range(96,101), range(73,78), range(48,53),range(23,28),range(5)]\n",
    "        self.get_sample()\n",
    "\n",
    "    def get_percentile(self, percentile):\n",
    "        percentile_value = self.df['similarity'].quantile((percentile/100), interpolation='lower')\n",
    "        return self.df[self.df['similarity'] == percentile_value]\n",
    "\n",
    "    def get_sample(self):\n",
    "        temp_list= []\n",
    "        for range in self.confirmation_list:\n",
    "            for position in range:\n",
    "                temp_list.append(self.get_percentile(position))\n",
    "        self.confirmation_df = pd.concat(temp_list)\n",
    "        self.confirmation_df.sort_values(by= 'similarity', ascending= False, inplace= True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = deduping_performance(nm, deduping.matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test.confirmation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.confirmation_df.loc[(test.confirmation_df['similarity'] >= lower_limit) & (test.confirmation_df['similarity'] <= upper_limit), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_duplicates = nm.merge(deduping.matched[['index','Ground Truth ID','similarity']].set_index('index'), left_index= True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detected_duplicates) - len(detected_duplicates[detected_duplicates['similarity'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detected_duplicates.drop_duplicates(subset=['Account Name'])) - len(detected_duplicates[detected_duplicates['similarity'] == 1].drop_duplicates(subset=['Account Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_duplicates.sort_values(by=['similarity'], ascending= False, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = deduping_performance(detected_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.confirmation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_duplicates.to_csv('Step 1 Matched_script.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "demandtools_output = glob.glob(link +'\\\\raw\\\\*step 3*')\n",
    "demandtools_output = [value for value in demandtools_output if 'Non_Match' not in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for file in demandtools_output:\n",
    "    temp_df = pd.read_csv(file, low_memory=False)\n",
    "    temp_list.append(temp_df)\n",
    "demandtools_output = pd.concat(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demandtools_output.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(demandtools_output['First and Last'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_duplicates[~detected_duplicates['Account Name'].isin(demandtools_output['First and Last'].unique().tolist())].drop_duplicates(subset=['Account Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demandtools_output[~demandtools_output['First and Last'].isin(detected_duplicates['Account Name'].unique().tolist())].drop_duplicates(subset=['First and Last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = detected_duplicates.merge(demandtools_output[['First and Last']].drop_duplicates(subset=['First and Last']), how='inner', left_on='Account Name', right_on='First and Last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d5c1156327dacead463cc502c55ebae8ce9c8c01979cf154173ff808e75bf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
